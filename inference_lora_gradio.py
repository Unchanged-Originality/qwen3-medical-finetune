import torch
import gradio as gr
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

# åŠ è½½æ¨¡å‹å’Œtokenizerï¼ˆå…¨å±€åŠ è½½ä¸€æ¬¡ï¼Œé¿å…é‡å¤åŠ è½½ï¼‰
print("æ­£åœ¨åŠ è½½æ¨¡å‹ï¼Œè¯·ç¨å€™...")
tokenizer = AutoTokenizer.from_pretrained("./Qwen/Qwen3-1.7B", use_fast=False, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained("./Qwen/Qwen3-1.7B", device_map="auto", torch_dtype=torch.bfloat16)
#model = PeftModel.from_pretrained(model, model_id="./output/Qwen3-1.7B/checkpoint-1084")
print("æ¨¡å‹åŠ è½½å®Œæˆï¼")

# ç¡®å®šè®¾å¤‡
if torch.backends.mps.is_available():
    device = "mps"
elif torch.cuda.is_available():
    device = "cuda"
else:
    device = "cpu"

def predict_with_model(instruction, user_input, max_new_tokens=2048):
    """
    ä½¿ç”¨æ¨¡å‹ç”Ÿæˆå›å¤
    """
    # æ„å»ºæ¶ˆæ¯
    messages = [
        {"role": "system", "content": instruction},
        {"role": "user", "content": user_input}
    ]
    
    # åº”ç”¨èŠå¤©æ¨¡æ¿
    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    model_inputs = tokenizer([text], return_tensors="pt").to(device)
    
    # ç”Ÿæˆå›å¤
    with torch.no_grad():
        generated_ids = model.generate(
            model_inputs.input_ids,
            max_new_tokens=max_new_tokens,
            do_sample=True,
            temperature=0.7,
            top_p=0.9,
        )
    
    # è§£ç å›å¤
    generated_ids = [
        output_ids[len(input_ids):] 
        for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
    ]
    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
    
    return response

def chat_interface(instruction, user_input, max_new_tokens):
    """
    Gradioç•Œé¢å¤„ç†å‡½æ•°
    """
    if not user_input.strip():
        return "è¯·è¾“å…¥æ‚¨çš„é—®é¢˜ã€‚"
    
    try:
        response = predict_with_model(instruction, user_input, int(max_new_tokens))
        return response
    except Exception as e:
        return f"ç”Ÿæˆå›å¤æ—¶å‡ºé”™: {str(e)}"

def create_example(example_num):
    """
    åˆ›å»ºç¤ºä¾‹
    """
    examples = [
        {
            "instruction": "ä½ æ˜¯ä¸€ä¸ªåŒ»å­¦ä¸“å®¶ï¼Œä½ éœ€è¦æ ¹æ®ç”¨æˆ·çš„é—®é¢˜ï¼Œç»™å‡ºå¸¦æœ‰æ€è€ƒçš„å›ç­”ã€‚",
            "input": "åŒ»ç”Ÿï¼Œæˆ‘æœ€è¿‘è¢«è¯Šæ–­ä¸ºç³–å°¿ç—…ï¼Œå¬è¯´ç¢³æ°´åŒ–åˆç‰©çš„é€‰æ‹©å¾ˆé‡è¦ï¼Œæˆ‘åº”è¯¥é€‰æ‹©ä»€ä¹ˆæ ·çš„ç¢³æ°´åŒ–åˆç‰©å‘¢ï¼Ÿ"
        },
        {
            "instruction": "ä½ æ˜¯ä¸€ä¸ªåŒ»å­¦ä¸“å®¶ï¼Œä½ éœ€è¦æ ¹æ®ç”¨æˆ·çš„é—®é¢˜ï¼Œç»™å‡ºå¸¦æœ‰æ€è€ƒçš„å›ç­”ã€‚",
            "input": "åŒ»ç”Ÿï¼Œæˆ‘æœ€è¿‘èƒƒéƒ¨ä¸é€‚ï¼Œå¬è¯´æœ‰å‡ ç§æŠ—æºƒç–¡è¯ç‰©å¯ä»¥æ²»ç–—ï¼Œæ‚¨èƒ½è¯¦ç»†ä»‹ç»ä¸€ä¸‹è¿™äº›è¯ç‰©çš„åˆ†ç±»ã€ä½œç”¨æœºåˆ¶ä»¥åŠå®ƒä»¬æ˜¯å¦‚ä½•å½±å“èƒƒé»è†œçš„ä¿æŠ¤ä¸æŸä¼¤å¹³è¡¡çš„å—ï¼Ÿ"
        },
        {
            "instruction": "ä½ æ˜¯ä¸€ä¸ªåŒ»å­¦ä¸“å®¶ï¼Œä½ éœ€è¦æ ¹æ®ç”¨æˆ·çš„é—®é¢˜ï¼Œç»™å‡ºå¸¦æœ‰æ€è€ƒçš„å›ç­”ã€‚",
            "input": "æˆ‘æœ€è¿‘è¢«è¯Šæ–­å‡ºæ‚£æœ‰æ·‹å·´ç˜¤ï¼ŒåŒ»ç”Ÿæåˆ°è¿™å¯èƒ½å¯¼è‡´å‘çƒ­ã€‚è¯·é—®è¿™æ˜¯ç”±äºæ·‹å·´ç˜¤ç»„ç»‡çš„åæ­»å’Œç»†èƒç ´åå¼•èµ·çš„å—ï¼Ÿå¦‚æœæ˜¯ï¼Œå…·ä½“æœºåˆ¶æ˜¯ä»€ä¹ˆï¼Ÿ"
        }
    ]
    
    if example_num < len(examples):
        return examples[example_num]["instruction"], examples[example_num]["input"]
    return "", ""

# åˆ›å»ºGradioç•Œé¢
with gr.Blocks(title="Qwen3-1.7BåŒ»å­¦åŠ©æ‰‹", theme=gr.themes.Soft()) as demo:
    gr.Markdown("""
    # ğŸ¥ Qwen3-1.7BåŒ»å­¦åŠ©æ‰‹-å¾®è°ƒå‰
   åŸºäºQwen3-1.7Bæ¨¡å‹å¾®è°ƒçš„åŒ»å­¦å¯¹è¯åŠ©æ‰‹ã€‚æ‚¨å¯ä»¥è¾“å…¥ç³»ç»ŸæŒ‡ä»¤å’Œæ‚¨çš„é—®é¢˜ï¼Œæ¨¡å‹ä¼šç”Ÿæˆè¯¦ç»†çš„å›ç­”ã€‚
    """)
    
    with gr.Row():
        with gr.Column(scale=1):
            gr.Markdown("### ç³»ç»ŸæŒ‡ä»¤è®¾ç½®")
            instruction_input = gr.Textbox(
                label="ç³»ç»ŸæŒ‡ä»¤",
                value="ä½ æ˜¯ä¸€ä¸ªåŒ»å­¦ä¸“å®¶ï¼Œä½ éœ€è¦æ ¹æ®ç”¨æˆ·çš„é—®é¢˜ï¼Œç»™å‡ºå¸¦æœ‰æ€è€ƒçš„å›ç­”ã€‚",
                lines=3,
                placeholder="è¯·è¾“å…¥ç³»ç»ŸæŒ‡ä»¤ï¼Œå®šä¹‰åŠ©æ‰‹çš„è§’è‰²å’Œå›ç­”é£æ ¼..."
            )
            
            gr.Markdown("### å‚æ•°è®¾ç½®")
            max_tokens_slider = gr.Slider(
                minimum=100,
                maximum=4096,
                value=2048,
                step=100,
                label="æœ€å¤§ç”Ÿæˆé•¿åº¦"
            )
            
            gr.Markdown("### ç¤ºä¾‹")
            with gr.Row():
                example1_btn = gr.Button("ç¤ºä¾‹1: ç³–å°¿ç—…é¥®é£Ÿå»ºè®®", variant="secondary", size="sm")
                example2_btn = gr.Button("ç¤ºä¾‹2: èƒƒæºƒç–¡è¯ç‰©ä»‹ç»", variant="secondary", size="sm")
                example3_btn = gr.Button("ç¤ºä¾‹3: æ·‹å·´ç˜¤æœºåˆ¶", variant="secondary", size="sm")
        
        with gr.Column(scale=2):
            gr.Markdown("### å¯¹è¯ç•Œé¢")
            user_input = gr.Textbox(
                label="æ‚¨çš„é—®é¢˜",
                lines=5,
                placeholder="è¯·è¾“å…¥æ‚¨çš„åŒ»å­¦æˆ–å¥åº·ç›¸å…³é—®é¢˜..."
            )
            
            submit_btn = gr.Button("ğŸš€ å‘é€", variant="primary")
            clear_btn = gr.Button("ğŸ§¹ æ¸…é™¤", variant="secondary")
            
            output = gr.Textbox(
                label="åŠ©æ‰‹å›å¤",
                lines=10,
                interactive=False
            )
    
    # ç¤ºä¾‹æŒ‰é’®çš„äº‹ä»¶å¤„ç†
    example1_btn.click(
        fn=lambda: create_example(0),
        outputs=[instruction_input, user_input]
    )
    
    example2_btn.click(
        fn=lambda: create_example(1),
        outputs=[instruction_input, user_input]
    )
    
    example3_btn.click(
        fn=lambda: create_example(2),
        outputs=[instruction_input, user_input]
    )
    
    # æäº¤æŒ‰é’®çš„äº‹ä»¶å¤„ç†
    submit_btn.click(
        fn=chat_interface,
        inputs=[instruction_input, user_input, max_tokens_slider],
        outputs=output
    )
    
    # æ¸…é™¤æŒ‰é’®çš„äº‹ä»¶å¤„ç†
    clear_btn.click(
        fn=lambda: ("", "", ""),
        outputs=[instruction_input, user_input, output]
    )
    
    # å›è½¦é”®æäº¤
    user_input.submit(
        fn=chat_interface,
        inputs=[instruction_input, user_input, max_tokens_slider],
        outputs=output
    )
    
    gr.Markdown("""
    ### ä½¿ç”¨è¯´æ˜
    1. åœ¨"ç³»ç»ŸæŒ‡ä»¤"ä¸­å®šä¹‰åŠ©æ‰‹çš„è§’è‰²
    2. åœ¨"æ‚¨çš„é—®é¢˜"ä¸­è¾“å…¥æ‚¨çš„é—®é¢˜
    3. ç‚¹å‡»"å‘é€"æŒ‰é’®æˆ–æŒ‰å›è½¦é”®è·å–å›ç­”
    4. å¯ä»¥ä½¿ç”¨å³ä¾§çš„ç¤ºä¾‹å¿«é€Ÿå¼€å§‹
    
    ### æ³¨æ„äº‹é¡¹
    - æœ¬æ¨¡å‹æä¾›çš„ä¿¡æ¯ä»…ä¾›å‚è€ƒï¼Œä¸èƒ½æ›¿ä»£ä¸“ä¸šåŒ»ç–—å»ºè®®
    - å¦‚æœ‰ä¸¥é‡å¥åº·é—®é¢˜ï¼Œè¯·åŠæ—¶å’¨è¯¢ä¸“ä¸šåŒ»ç”Ÿ
    - æ¨¡å‹å›å¤å¯èƒ½å­˜åœ¨å»¶è¿Ÿï¼Œè¯·è€å¿ƒç­‰å¾…
    """)

if __name__ == "__main__":
    # å¯åŠ¨Gradioç•Œé¢
    demo.launch(
        server_name="0.0.0.0",  # å…è®¸å¤–éƒ¨è®¿é—®
        server_port=7860,        # ç«¯å£å·
        share=False,             # æ˜¯å¦åˆ›å»ºå…¬å¼€é“¾æ¥
        debug=False              # è°ƒè¯•æ¨¡å¼
    )